\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{fullpage}

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{siunitx}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}

\usepackage[colorlinks=false,pdfborder={0 0 0}]{hyperref}
\usepackage[all]{hypcap}


\title{Astro 585: HW 7}
\author{Codename: The Maxwell-JÃ¼ttner Distribution}



\begin{document}


\maketitle

My git repository is here: \url{https://github.com/hsgg/astro585}, clone URL
\url{https://github.com/hsgg/astro585.git}.


\section{PBS, Clusters, LionXV}

$\pi \approx \num{3.140996000000}$. Hm, $\pi$\ldots

The following, busy plot shows the computation time as function of number of
worker threads, for both the loop and the map operations with \num{e8}
iterations. For each operation and number of workers, the code was run three
times -- all on one node, all on different nodes, and finally any combination
that the pbs system saw fit to use.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
	ymin=0,
	xlabel=Number of worker processes,
	ylabel=Computation time in wall-seconds,
        width=0.8\textwidth,
        height=10cm,
        legend columns=2,
        legend to name=firstlegend
      ]

      \addplot table[x index=0,y index=1]{q1_diffnodes.dat};
      \addlegendentry{loop, diff nodes};

      \addplot table[x index=0,y index=2]{q1_diffnodes.dat};
      \addlegendentry{map, diff nodes};


      \addplot table[x index=0,y index=1]{q1_samenode.dat};
      \addlegendentry{loop, same node};

      \addplot table[x index=0,y index=2]{q1_samenode.dat};
      \addlegendentry{map, same node};


      \addplot table[x index=0,y index=1]{q1_anynode.dat};
      \addlegendentry{loop, any node};

      \addplot table[x index=0,y index=2]{q1_anynode.dat};
      \addlegendentry{map, any node};
    \end{axis}
  \end{tikzpicture}
  \ref{firstlegend}
\end{center}

Of course, it took longest to get started the job that needed 8 cores on a
single node. Contention with others users is highest there.

Above 4 processors, the scaling is about linear, with not much to be gained
from more processors.

I find a big difference between using map versus the hand-coded looping
implementation, but not all that much between running on one node versus
spreading it out.

For reference, these are the commands to run it:
\begin{center}
\begin{verbatim}
    $ cd hw7/
    $ ./mkqsubscript.sh > q1_script.sh
    $ chmod +x q1_script.sh
    $ ./q1_script.sh
\end{verbatim}
\end{center}
Then wait. To extract the numbers:
\begin{center}
\begin{verbatim}
    $ ./Q1_get_result.sh
\end{verbatim}
\end{center}
Then recompile the \LaTeX{} document \path{soln.tex} to recreate the graph.



\section{CUDA}

For this I used \url{tesla.rcc.psu.edu}.

\begin{center}
  \begin{tikzpicture}
    \begin{loglogaxis}[
	xlabel=Size of load,
	ylabel=Computation time in wall-seconds,
        width=0.8\textwidth,
        height=10cm,
        legend columns=2,
        legend to name=secondlegend
      ]
      \addplot table[x index=0,y index=1]{q2.dat};
      \addlegendentry{upload};

      \addplot table[x index=0,y index=2]{q2.dat};
      \addlegendentry{on GPU};


      \addplot table[x index=0,y index=3]{q2.dat};
      \addlegendentry{download};

      \addplot table[x index=0,y index=4]{q2.dat};
      \addlegendentry{CPU summation};

      %\addplot table[x index=0,y index=5]{q2.dat};
      %\addlegendentry{Total};
    \end{loglogaxis}
  \end{tikzpicture}
  \ref{secondlegend}
\end{center}

For small data sets the GPU takes much of the time, although the data upload to
the GPU also takes some time. I find it surprising that it takes longer than
for larger datasets. Perhaps if I ran it multiple times, there would be a large
scatter caused by interference from other processes.

For large datasets the GPU time becomes negligible, and the up- and download
times become dominant, as the GPU can distribute the work efiiciently among its
small cores.

Performing the summation on the GPU potentially increases the performance
significantly, as the download to the CPU will essentially disappear. However,
since the data still needs to be accessed for the summation, I expect the GPU
to take more time doing the calculation than the CPU, so the speedup will be
less. It depends on whether the CPU is the bottleneck in the download, or the
GPU.

For reference, the commands to get the plots are:
\begin{center}
\begin{verbatim}
    $ cd hw7
    $ ./HW7_Q2.jl > q2.out  # must be run where GPU is available
    $ ./Q2_parse_script.sh < q2.out > q2.dat
\end{verbatim}
\end{center}
Then recompile the \LaTeX{} document \path{soln.tex} to recreate the graph.

It seems to frequently and randomly fail at the synchronization points.


\subsection{Summation on the GPU}

Ah, a nice problem! The summations agree on the answer, something like
\num{0.6826894993594692}, which is pretty close to what it should be.



\section{The rest}

Hm\ldots somehow I cannot convince myself to do these\ldots


\end{document}

% vim: set sw=2 sts=2 et :
